# H∆Ø·ªöNG D·∫™N SINH VI√äN - COMPUTER VISION CAMERA PROJECT

## T·ªïng quan d·ª± √°n

D·ª± √°n n√†y x√¢y d·ª±ng h·ªá th·ªëng camera gi√°m s√°t b√£i ƒë·ªó xe v·ªõi kh·∫£ nƒÉng:
- K·∫øt n·ªëi 2 camera (RTSP/HTTP/USB)
- X·ª≠ l√Ω ·∫£nh theo t·ª´ng b∆∞·ªõc (preprocessing, segmentation, tracking, OCR bi·ªÉn s·ªë xe)
- Hi·ªÉn th·ªã k·∫øt qu·∫£ real-time tr√™n web interface

## C·∫•u tr√∫c code

```
‚îú‚îÄ‚îÄ app.py              # Flask web server (ƒë√£ ho√†n thi·ªán)
‚îú‚îÄ‚îÄ camera.py           # Video camera handler (ƒë√£ ho√†n thi·ªán)
‚îú‚îÄ‚îÄ process.py          # Image processing (SINH VI√äN C·∫¶N HO√ÄN THI·ªÜN)
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ index.html      # Web interface (ƒë√£ ho√†n thi·ªán)
‚îî‚îÄ‚îÄ static/
    ‚îú‚îÄ‚îÄ main.js         # Frontend JavaScript (ƒë√£ ho√†n thi·ªán)
    ‚îî‚îÄ‚îÄ style.css       # Styling (ƒë√£ ho√†n thi·ªán)
```

## Nhi·ªám v·ª• c·ªßa sinh vi√™n

**Sinh vi√™n c·∫ßn ho√†n thi·ªán c√°c ph∆∞∆°ng th·ª©c trong file `process.py`** theo t·ª´ng b∆∞·ªõc trong `ProjectProgress.txt`

---

## H∆Ø·ªöNG D·∫™N T·ª™NG B∆Ø·ªöC

### STEP 1: Basic Image Capture (Weeks 1-2)

#### Ph∆∞∆°ng th·ª©c: `capture_and_save_image()`

**Y√™u c·∫ßu:** L∆∞u ·∫£nh v√†o th∆∞ m·ª•c `CapturedImage/`

**G·ª£i √Ω:**
```python
def capture_and_save_image(self, bgr_img, filename):
    if bgr_img is None:
        return False
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
    import os
    os.makedirs('CapturedImage', exist_ok=True)
    
    # L∆∞u ·∫£nh
    filepath = os.path.join('CapturedImage', filename)
    return cv2.imwrite(filepath, bgr_img)
```

**Test:** Ki·ªÉm tra xem file ·∫£nh c√≥ ƒë∆∞·ª£c t·∫°o trong th∆∞ m·ª•c `CapturedImage/` kh√¥ng

---

### STEP 2: Image Preprocessing (Week 3)

#### Ph∆∞∆°ng th·ª©c c·∫ßn ho√†n thi·ªán:
1. `convert_to_grayscale()`
2. `apply_gaussian_filter()`
3. `detect_edges_canny()`
4. `preprocess_image()`

**Y√™u c·∫ßu:** Preprocessing pipeline v·ªõi grayscale ‚Üí Gaussian blur ‚Üí Canny edge

**G·ª£i √Ω cho `convert_to_grayscale()`:**
```python
def convert_to_grayscale(self, bgr_img):
    return cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)
```

**G·ª£i √Ω cho `apply_gaussian_filter()`:**
```python
def apply_gaussian_filter(self, img, kernel_size=(5, 5), sigma=1.0):
    return cv2.GaussianBlur(img, kernel_size, sigma)
```

**G·ª£i √Ω cho `detect_edges_canny()`:**
```python
def detect_edges_canny(self, img, threshold1=50, threshold2=150):
    return cv2.Canny(img, threshold1, threshold2)
```

**G·ª£i √Ω cho `preprocess_image()`:**
```python
def preprocess_image(self, bgr_img):
    gray = self.convert_to_grayscale(bgr_img)
    filtered = self.apply_gaussian_filter(gray)
    edges = self.detect_edges_canny(filtered)
    
    return {
        'grayscale': gray,
        'filtered': filtered,
        'edges': edges
    }
```

**Test:** B·∫•m Capture v√† ki·ªÉm tra k·∫øt qu·∫£ edges c√≥ hi·ªÉn th·ªã ƒë√∫ng kh√¥ng

---

### STEP 3: Color Space Conversion & Segmentation (Week 4)

#### Ph∆∞∆°ng th·ª©c c·∫ßn ho√†n thi·ªán:
1. `convert_to_hsv()`
2. `segment_by_color()`
3. `apply_morphology()`

**Y√™u c·∫ßu:** Chuy·ªÉn ƒë·ªïi HSV v√† segmentation theo m√†u s·∫Øc

**G·ª£i √Ω cho `convert_to_hsv()`:**
```python
def convert_to_hsv(self, bgr_img):
    return cv2.cvtColor(bgr_img, cv2.COLOR_BGR2HSV)
```

**G·ª£i √Ω cho `segment_by_color()`:**
```python
def segment_by_color(self, bgr_img, lower_bound, upper_bound):
    hsv = self.convert_to_hsv(bgr_img)
    mask = cv2.inRange(hsv, lower_bound, upper_bound)
    return mask
```

**V√≠ d·ª• s·ª≠ d·ª•ng:** Segment xe m√†u ƒë·ªè
```python
# Trong process_frame(), n·∫øu step == 'segment':
lower_red = np.array([0, 100, 100])
upper_red = np.array([10, 255, 255])
mask = self.segment_by_color(bgr_img, lower_red, upper_red)
```

**G·ª£i √Ω cho `apply_morphology()`:**
```python
def apply_morphology(self, binary_img, operation='close', kernel_size=(5, 5)):
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)
    
    if operation == 'erode':
        return cv2.erode(binary_img, kernel)
    elif operation == 'dilate':
        return cv2.dilate(binary_img, kernel)
    elif operation == 'open':
        return cv2.morphologyEx(binary_img, cv2.MORPH_OPEN, kernel)
    elif operation == 'close':
        return cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, kernel)
    return binary_img
```

**Test:** Segment m·ªôt m√†u c·ª• th·ªÉ v√† xem k·∫øt qu·∫£ mask

---

### STEP 4: Homography and Calibration (Week 5)

#### Ph∆∞∆°ng th·ª©c c·∫ßn ho√†n thi·ªán:
1. `calibrate_camera()`
2. `undistort_image()`
3. `compute_homography()`
4. `apply_perspective_transform()`

**Y√™u c·∫ßu:** Calibrate camera v√† correct perspective

**G·ª£i √Ω cho `calibrate_camera()`:**
```python
def calibrate_camera(self, calibration_images, pattern_size=(9, 6)):
    objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)
    objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)
    
    objpoints = []  # 3D points
    imgpoints = []  # 2D points
    
    for img in calibration_images:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)
        
        if ret:
            objpoints.append(objp)
            imgpoints.append(corners)
    
    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(
        objpoints, imgpoints, gray.shape[::-1], None, None
    )
    
    self.camera_matrix = mtx
    self.dist_coeffs = dist
    
    return mtx, dist, rvecs, tvecs
```

**G·ª£i √Ω cho `undistort_image()`:**
```python
def undistort_image(self, bgr_img):
    if self.camera_matrix is None or self.dist_coeffs is None:
        raise ValueError("Camera not calibrated yet")
    
    return cv2.undistort(bgr_img, self.camera_matrix, self.dist_coeffs)
```

**Test:** Ch·ª•p ·∫£nh checkerboard, calibrate v√† so s√°nh tr∆∞·ªõc/sau undistort

---

### STEP 5: Region of Interest Detection (Weeks 6-7)

#### Ph∆∞∆°ng th·ª©c c·∫ßn ho√†n thi·ªán:
1. `detect_corners_harris()`
2. `detect_features_orb()`
3. `detect_features_sift()`
4. `crop_vehicle_roi()`

**Y√™u c·∫ßu:** Feature detection ƒë·ªÉ t√¨m v√πng xe

**G·ª£i √Ω cho `detect_features_orb()`:**
```python
def detect_features_orb(self, img, n_features=500):
    orb = cv2.ORB_create(nfeatures=n_features)
    keypoints, descriptors = orb.detectAndCompute(img, None)
    return keypoints, descriptors
```

**G·ª£i √Ω cho `crop_vehicle_roi()`:**
```python
def crop_vehicle_roi(self, bgr_img, roi_coords=None):
    if roi_coords is None:
        # Auto-detect ROI using edge detection
        gray = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if len(contours) > 0:
            # Get largest contour
            largest = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest)
            roi_coords = (x, y, w, h)
        else:
            # Default to center crop
            h, w = bgr_img.shape[:2]
            roi_coords = (w//4, h//4, w//2, h//2)
    
    x, y, w, h = roi_coords
    return bgr_img[y:y+h, x:x+w].copy()
```

**Test:** Capture ·∫£nh c√≥ xe v√† ki·ªÉm tra ROI c√≥ crop ƒë√∫ng v√πng xe kh√¥ng

---

### STEP 6: Motion Detection (Weeks 9-10)

#### Ph∆∞∆°ng th·ª©c c·∫ßn ho√†n thi·ªán:
1. `detect_motion_frame_diff()`
2. `compute_optical_flow_lk()`
3. `compute_optical_flow_farneback()`

**Y√™u c·∫ßu:** Detect chuy·ªÉn ƒë·ªông qua frame differencing v√† optical flow

**G·ª£i √Ω cho `detect_motion_frame_diff()`:**
```python
def detect_motion_frame_diff(self, current_frame, threshold=25):
    if self.previous_frame is None:
        self.previous_frame = current_frame.copy()
        return None
    
    # Convert to grayscale
    gray1 = cv2.cvtColor(self.previous_frame, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
    
    # Absolute difference
    diff = cv2.absdiff(gray1, gray2)
    
    # Threshold
    _, motion_mask = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)
    
    # Update previous frame
    self.previous_frame = current_frame.copy()
    
    return motion_mask
```

**G·ª£i √Ω cho `compute_optical_flow_lk()`:**
```python
def compute_optical_flow_lk(self, prev_gray, curr_gray, prev_points):
    lk_params = dict(
        winSize=(15, 15),
        maxLevel=2,
        criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)
    )
    
    next_points, status, error = cv2.calcOpticalFlowPyrLK(
        prev_gray, curr_gray, prev_points, None, **lk_params
    )
    
    return next_points, status, error
```

**Test:** B·∫•m Capture nhi·ªÅu l·∫ßn li√™n ti·∫øp v√† quan s√°t motion mask

---

### STEP 7: Object Tracking (Weeks 11-12)

#### Ph∆∞∆°ng th·ª©c c·∫ßn ho√†n thi·ªán:
1. `initialize_kalman_filter()`
2. `update_kalman_filter()`
3. `track_objects()`

**Y√™u c·∫ßu:** Track xe s·ª≠ d·ª•ng Kalman filter

**G·ª£i √Ω cho `initialize_kalman_filter()`:**
```python
def initialize_kalman_filter(self):
    kalman = cv2.KalmanFilter(4, 2)  # 4 states (x, y, dx, dy), 2 measurements (x, y)
    
    # Transition matrix
    kalman.transitionMatrix = np.array([
        [1, 0, 1, 0],
        [0, 1, 0, 1],
        [0, 0, 1, 0],
        [0, 0, 0, 1]
    ], np.float32)
    
    # Measurement matrix
    kalman.measurementMatrix = np.array([
        [1, 0, 0, 0],
        [0, 1, 0, 0]
    ], np.float32)
    
    kalman.processNoiseCov = np.eye(4, dtype=np.float32) * 0.03
    kalman.measurementNoiseCov = np.eye(2, dtype=np.float32) * 1
    
    return kalman
```

**G·ª£i √Ω cho `update_kalman_filter()`:**
```python
def update_kalman_filter(self, kalman, measurement):
    # Correct with measurement
    kalman.correct(np.array([[np.float32(measurement[0])], 
                              [np.float32(measurement[1])]]))
    
    # Predict next state
    prediction = kalman.predict()
    
    return (int(prediction[0]), int(prediction[1]))
```

**Test:** Track m·ªôt object ƒë∆°n gi·∫£n qua nhi·ªÅu frames

---

### STEP 8: License Plate Localization (Weeks 6-8, 13-14)

#### Ph∆∞∆°ng th·ª©c c·∫ßn ho√†n thi·ªán:
1. `locate_license_plate()`
2. `enhance_plate_image()`

**Y√™u c·∫ßu:** T√¨m v·ªã tr√≠ bi·ªÉn s·ªë xe

**G·ª£i √Ω cho `locate_license_plate()`:**
```python
def locate_license_plate(self, vehicle_img):
    # Preprocessing
    gray = cv2.cvtColor(vehicle_img, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    
    # Edge detection
    edges = cv2.Canny(blur, 50, 200)
    
    # Find contours
    contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    
    # Filter by aspect ratio (license plates are wider than tall)
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        aspect_ratio = w / float(h)
        
        # Typical license plate aspect ratio: 2-5
        if 2.0 <= aspect_ratio <= 5.0 and w > 50 and h > 15:
            # Found potential plate
            plate = vehicle_img[y:y+h, x:x+w].copy()
            return plate
    
    return None
```

**G·ª£i √Ω cho `enhance_plate_image()`:**
```python
def enhance_plate_image(self, plate_img):
    # Resize to standard size
    plate = cv2.resize(plate_img, (300, 100))
    
    # Convert to grayscale
    gray = cv2.cvtColor(plate, cv2.COLOR_BGR2GRAY)
    
    # Adaptive threshold
    thresh = cv2.adaptiveThreshold(
        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
        cv2.THRESH_BINARY, 11, 2
    )
    
    # Morphological closing
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
    
    return cleaned
```

**Test:** Ch·ª•p ·∫£nh xe c√≥ bi·ªÉn s·ªë r√µ r√†ng v√† ki·ªÉm tra c√≥ detect ƒë∆∞·ª£c kh√¥ng

---

### STEP 9: License Plate Character Recognition (Weeks 13-14)

#### Ph∆∞∆°ng th·ª©c c·∫ßn ho√†n thi·ªán:
1. `recognize_plate_text()`
2. `segment_characters()`
3. `train_character_classifier()`
4. `classify_character()`

**Y√™u c·∫ßu:** OCR bi·ªÉn s·ªë xe

**C√†i ƒë·∫∑t pytesseract:**
```bash
pip install pytesseract
# Download Tesseract t·ª´: https://github.com/UB-Mannheim/tesseract/wiki
```

**G·ª£i √Ω cho `recognize_plate_text()`:**
```python
def recognize_plate_text(self, plate_img):
    import pytesseract
    
    # Config for license plate (only alphanumeric)
    config = '--oem 3 --psm 7 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
    
    text = pytesseract.image_to_string(plate_img, config=config)
    
    # Post-process: remove special characters and spaces
    text = ''.join(filter(str.isalnum, text))
    
    return text
```

**G·ª£i √Ω cho `segment_characters()`:**
```python
def segment_characters(self, plate_img):
    # Find contours of characters
    contours, _ = cv2.findContours(plate_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # Sort contours left to right
    contours = sorted(contours, key=lambda c: cv2.boundingRect(c)[0])
    
    characters = []
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        
        # Filter noise (too small)
        if w > 5 and h > 15:
            char_img = plate_img[y:y+h, x:x+w]
            characters.append(char_img)
    
    return characters
```

**Test:** Nh·∫≠n d·∫°ng bi·ªÉn s·ªë v√† hi·ªÉn th·ªã text l√™n m√†n h√¨nh

---

### STEP 10: System Integration (Week 14)

#### Ph∆∞∆°ng th·ª©c c·∫ßn ho√†n thi·ªán:
1. `process_frame()` - Pipeline ho√†n ch·ªânh
2. `visualize_results()` - V·∫Ω k·∫øt qu·∫£ l√™n ·∫£nh

**Y√™u c·∫ßu:** T√≠ch h·ª£p t·∫•t c·∫£ c√°c b∆∞·ªõc

**G·ª£i √Ω cho `process_frame()` - ho√†n ch·ªânh:**
```python
def process_frame(self, bgr_img, step='all'):
    if bgr_img is None:
        raise ValueError("Input frame is None")
    
    start_time = time.perf_counter()
    results = {}
    processed_img = bgr_img.copy()
    
    if step == 'preprocess' or step == 'all':
        # Step 2: Preprocessing
        preprocess_results = self.preprocess_image(bgr_img)
        results['preprocess'] = preprocess_results
        
        # Visualize edges
        edges_color = cv2.cvtColor(preprocess_results['edges'], cv2.COLOR_GRAY2BGR)
        processed_img = edges_color
    
    elif step == 'segment':
        # Step 3: Segmentation
        lower = np.array([0, 100, 100])
        upper = np.array([10, 255, 255])
        mask = self.segment_by_color(bgr_img, lower, upper)
        
        # Apply to original image
        segmented = cv2.bitwise_and(bgr_img, bgr_img, mask=mask)
        results['segmentation'] = {'mask': mask}
        processed_img = segmented
    
    elif step == 'roi':
        # Step 5: ROI detection
        cropped = self.crop_vehicle_roi(bgr_img)
        results['roi'] = {'cropped': cropped}
        processed_img = cropped
    
    elif step == 'motion':
        # Step 6: Motion detection
        motion_mask = self.detect_motion_frame_diff(bgr_img)
        if motion_mask is not None:
            motion_color = cv2.cvtColor(motion_mask, cv2.COLOR_GRAY2BGR)
            processed_img = motion_color
            results['motion'] = {'has_motion': np.sum(motion_mask) > 1000}
    
    elif step == 'license_plate':
        # Steps 8-9: License plate
        vehicle_roi = self.crop_vehicle_roi(bgr_img)
        plate = self.locate_license_plate(vehicle_roi)
        
        if plate is not None:
            enhanced = self.enhance_plate_image(plate)
            text = self.recognize_plate_text(enhanced)
            
            results['license_plate'] = {
                'plate_image': plate,
                'text': text
            }
            processed_img = enhanced
        else:
            results['license_plate'] = {'text': 'NOT FOUND'}
    
    process_time_ms = (time.perf_counter() - start_time) * 1000
    
    return processed_img, results, process_time_ms
```

**G·ª£i √Ω cho `visualize_results()`:**
```python
def visualize_results(self, bgr_img, results):
    annotated = bgr_img.copy()
    
    # Draw license plate text if available
    if 'license_plate' in results and 'text' in results['license_plate']:
        text = results['license_plate']['text']
        cv2.putText(annotated, f"Plate: {text}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    
    # Draw tracking boxes if available
    if 'tracked_objects' in results:
        for obj in results['tracked_objects']:
            x, y, w, h = obj['bbox']
            cv2.rectangle(annotated, (x, y), (x+w, y+h), (0, 255, 0), 2)
            cv2.putText(annotated, f"ID: {obj['id']}", (x, y-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    
    return annotated
```

---

## C√ÅCH TEST H·ªÜ TH·ªêNG

### 1. Test c∆° b·∫£n:
```bash
# Ch·∫°y Flask app
python app.py

# M·ªü browser: http://localhost:5000
# Connect camera v√† b·∫•m Capture
```

### 2. Test t·ª´ng b∆∞·ªõc:
- **Step 2 (Preprocess):** B·∫•m Capture ‚Üí Xem edges trong "Fragment (processed)"
- **Step 3 (Segment):** Thay ƒë·ªïi m√†u trong code ‚Üí B·∫•m Capture ‚Üí Xem mask
- **Step 5 (ROI):** B·∫•m Capture ‚Üí Xem v√πng xe ƒë∆∞·ª£c crop
- **Step 8-9 (License Plate):** Ch·ª•p xe c√≥ bi·ªÉn s·ªë ‚Üí Xem OCR result

### 3. Modify frontend ƒë·ªÉ ch·ªçn step:
C√≥ th·ªÉ th√™m dropdown trong `index.html` ƒë·ªÉ ch·ªçn step:
```html
<select id="step-1">
  <option value="all">All</option>
  <option value="preprocess">Preprocess</option>
  <option value="segment">Segment</option>
  <option value="roi">ROI</option>
  <option value="motion">Motion</option>
  <option value="license_plate">License Plate</option>
</select>
```

V√† update `main.js`:
```javascript
async function capture(cam_id){
  const step = document.getElementById(`step-${cam_id}`).value;
  const res = await fetch('/capture', {
    method: 'POST',
    headers: {'Content-Type':'application/json'},
    body: JSON.stringify({cam_id: cam_id, step: step})
  });
  // ... rest of code
}
```

---

## TIPS & BEST PRACTICES

1. **Debug t·ª´ng b∆∞·ªõc:** Kh√¥ng n√™n implement h·∫øt t·∫•t c·∫£ m·ªôt l√∫c. Test t·ª´ng method m·ªôt.

2. **Visualize intermediate results:** Lu√¥n hi·ªÉn th·ªã k·∫øt qu·∫£ trung gian ƒë·ªÉ debug d·ªÖ h∆°n.

3. **Handle errors:** Th√™m try-except ƒë·ªÉ catch l·ªói v√† log ra.

4. **Tune parameters:** C√°c threshold, kernel size c·∫ßn tune theo t·ª´ng camera/lighting.

5. **Save results:** L∆∞u ·∫£nh k·∫øt qu·∫£ v√†o th∆∞ m·ª•c ƒë·ªÉ so s√°nh.

6. **Document code:** Th√™m comments gi·∫£i th√≠ch logic.

---

## T√ÄI LI·ªÜU THAM KH·∫¢O

- OpenCV Documentation: https://docs.opencv.org/
- OpenCV Python Tutorials: https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html
- Tesseract OCR: https://github.com/tesseract-ocr/tesseract
- Flask Documentation: https://flask.palletsprojects.com/

---

## ƒê√ÅNH GI√Å CU·ªêI K·ª≤

Sinh vi√™n c·∫ßn:
1. **Demo h·ªá th·ªëng ho√†n ch·ªânh** (70%):
   - K·∫øt n·ªëi 2 camera
   - X·ª≠ l√Ω ·∫£nh real-time
   - Nh·∫≠n d·∫°ng bi·ªÉn s·ªë xe

2. **B√°o c√°o k·ªπ thu·∫≠t** (20%):
   - M√¥ t·∫£ ki·∫øn tr√∫c h·ªá th·ªëng
   - Gi·∫£i th√≠ch thu·∫≠t to√°n
   - K·∫øt qu·∫£ th·ª≠ nghi·ªám

3. **Code quality** (10%):
   - Clean code
   - Comments ƒë·∫ßy ƒë·ªß
   - OOP principles

**Ch√∫c c√°c b·∫°n th√†nh c√¥ng!** üöÄ
